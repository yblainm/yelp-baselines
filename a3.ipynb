{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import re, numpy as np, os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(path):\n",
    "\t\"\"\"\n",
    "\t:param text: raw text file\n",
    "\t:type text: str\n",
    "\t:return list of str lines, list of int labels\n",
    "\t\"\"\"\n",
    "\traw = []    # Raw lines\n",
    "\tlines = []  # Tokenized\n",
    "\tlabels = [] # Labels\n",
    "\twith open(path, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "\t\ttext = f.readlines()\n",
    "\t\tfor line in text:\n",
    "\t\t\tif len(line) > 0:\n",
    "\t\t\t\traw.append(prepare_text(line[:-3]))     # :-2 to exclude \\t\n",
    "\t\t\t\tlines.append(prepare_text(line[:-3], token=True))\n",
    "\t\t\t\tlabels.append(int(line[-2]))            # The label\n",
    "\treturn raw, lines, labels\n",
    "\n",
    "\n",
    "def prepare_text(text, token=False):\n",
    "\treturn word_tokenize(re.sub(r'\\W', ' ', text.lower().replace('\\'',''))) \\\n",
    "\t\tif token else re.sub(r'\\W', ' ', text.lower().replace('\\'',''))\n",
    "\n",
    "\n",
    "def datafromfile(path):\n",
    "\twith open(path, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "\t\ttext = f.read()\n",
    "\t\tf.close()\n",
    "\n",
    "\traw, lines, labels = prepare_data(path)\n",
    "\treturn text, raw, lines, labels\n",
    "\n",
    "\n",
    "def export_vocabulary(path, raw, vectorizer):\n",
    "\t\"\"\"\n",
    "\n",
    "\t:param raw: entire raw corpus text string\n",
    "\t:param vectorizer: pre-fit vectorizer\n",
    "\t:type vectorizer CountVectorizer\n",
    "\t:type raw list\n",
    "\t\"\"\"\n",
    "\n",
    "\tx = vectorizer.transform(raw)\n",
    "\t# [word, id, frequency]\n",
    "\tfeaturenames = vectorizer.get_feature_names()\n",
    "\tindices = { k : featurenames.index(k) for k in vectorizer.vocabulary_.keys()}\n",
    "\tvocab = [k+\" \"+str(indices[k])+\" \"+str(x[0,indices[k]])+\"\\n\" for k,v in vectorizer.vocabulary_.items()]\n",
    "\twith open(path, \"w\") as f:\n",
    "\t\tf.writelines(vocab)\n",
    "\t\tf.close()\n",
    "\treturn vocab\n",
    "\n",
    "\n",
    "def export_data(fname, x_tokens, vectorizer, y):\n",
    "\t'''\n",
    "\n",
    "\t:param vectorizer:\n",
    "\t:type vectorizer CountVectorizer\n",
    "\t:return:\n",
    "\t'''\n",
    "\tout = \"\"\n",
    "\tfor i in range(0, len(y)):\n",
    "\t\tfor token in x_tokens[i]:\n",
    "\t\t\tidx = vectorizer.vocabulary_.get(token)\n",
    "\t\t\tif idx:\n",
    "\t\t\t\tout += str(vectorizer.vocabulary_[token])\n",
    "\t\t\t\tout += \" \"\n",
    "\t\tout = out[:-1] + \"\\t\" + str(y[i]) + \"\\n\"\n",
    "\twith open(fname, mode=\"w\", encoding=\"utf-8\") as f:\n",
    "\t\tf.write(out)\n",
    "\t\tf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp_train_raw, yelp_train_rawlines, yelp_train_tokenlines, yelp_bbow_train_y = \\\n",
    "\tdatafromfile(os.path.join(\"data\",\"yelp-train.txt\"))\n",
    "yelp_valid_raw, yelp_valid_rawlines, yelp_valid_tokenlines, yelp_bbow_valid_y = \\\n",
    "\tdatafromfile(os.path.join(\"data\",\"yelp-valid.txt\"))\n",
    "yelp_test_raw, yelp_test_rawlines, yelp_test_tokenlines, yelp_bbow_test_y = \\\n",
    "\tdatafromfile(os.path.join(\"data\",\"yelp-test.txt\"))\n",
    "\n",
    "yelp_vectorizer_bbow = CountVectorizer(max_features=10000, binary=True)\n",
    "yelp_bbow_train_x = yelp_vectorizer_bbow.fit_transform(np.array(yelp_train_rawlines))\n",
    "yelp_bbow_valid_x = yelp_vectorizer_bbow.transform(np.array(yelp_valid_rawlines))\n",
    "yelp_bbow_test_x = yelp_vectorizer_bbow.transform(np.array(yelp_test_rawlines))\n",
    "yelp_bbow_train_y = np.array(yelp_bbow_train_y)\n",
    "yelp_bbow_valid_y = np.array(yelp_bbow_valid_y)\n",
    "yelp_bbow_test_y = np.array(yelp_bbow_test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp_train_raw, yelp_train_rawlines, yelp_train_tokenlines, yelp_train_y = \\\n",
    "\tdatafromfile(os.path.join(\"data\",\"yelp-train.txt\"))\n",
    "yelp_valid_raw, yelp_valid_rawlines, yelp_valid_tokenlines, yelp_valid_y = \\\n",
    "\tdatafromfile(os.path.join(\"data\",\"yelp-valid.txt\"))\n",
    "yelp_test_raw, yelp_test_rawlines, yelp_test_tokenlines, yelp_test_y = \\\n",
    "\tdatafromfile(os.path.join(\"data\",\"yelp-test.txt\"))\n",
    "\n",
    "yelp_vectorizer = CountVectorizer(max_features=10000)\n",
    "yelp_train_x = yelp_vectorizer.fit_transform(np.array(yelp_train_rawlines))\n",
    "yelp_valid_x = yelp_vectorizer.transform(np.array(yelp_valid_rawlines))\n",
    "yelp_test_x = yelp_vectorizer.transform(np.array(yelp_test_rawlines))\n",
    "yelp_train_y = np.array(yelp_train_y)\n",
    "yelp_valid_y = np.array(yelp_valid_y)\n",
    "yelp_test_y = np.array(yelp_test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMDB_train_raw, IMDB_train_rawlines, IMDB_train_tokenlines, IMDB_bbow_train_y = \\\n",
    "\tdatafromfile(os.path.join(\"data\",\"IMDB-train.txt\"))\n",
    "IMDB_valid_raw, IMDB_valid_rawlines, IMDB_valid_tokenlines, IMDB_bbow_valid_y = \\\n",
    "\tdatafromfile(os.path.join(\"data\",\"IMDB-valid.txt\"))\n",
    "IMDB_test_raw, IMDB_test_rawlines, IMDB_test_tokenlines, IMDB_bbow_test_y = \\\n",
    "\tdatafromfile(os.path.join(\"data\",\"IMDB-test.txt\"))\n",
    "\n",
    "IMDB_vectorizer_bbow = CountVectorizer(max_features=10000, binary=True)\n",
    "IMDB_bbow_train_x = IMDB_vectorizer_bbow.fit_transform(np.array(IMDB_train_rawlines))\n",
    "IMDB_bbow_valid_x = IMDB_vectorizer_bbow.transform(np.array(IMDB_valid_rawlines))\n",
    "IMDB_bbow_test_x = IMDB_vectorizer_bbow.transform(np.array(IMDB_test_rawlines))\n",
    "IMDB_bbow_train_y = np.array(IMDB_bbow_train_y)\n",
    "IMDB_bbow_valid_y = np.array(IMDB_bbow_valid_y)\n",
    "IMDB_bbow_test_y = np.array(IMDB_bbow_test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMDB_train_raw, IMDB_train_rawlines, IMDB_train_tokenlines, IMDB_train_y = \\\n",
    "\tdatafromfile(os.path.join(\"data\",\"IMDB-train.txt\"))\n",
    "IMDB_valid_raw, IMDB_valid_rawlines, IMDB_valid_tokenlines, IMDB_valid_y = \\\n",
    "\tdatafromfile(os.path.join(\"data\",\"IMDB-valid.txt\"))\n",
    "IMDB_test_raw, IMDB_test_rawlines, IMDB_test_tokenlines, IMDB_test_y = \\\n",
    "\tdatafromfile(os.path.join(\"data\",\"IMDB-test.txt\"))\n",
    "\n",
    "IMDB_vectorizer = CountVectorizer(max_features=10000)\n",
    "IMDB_train_x = IMDB_vectorizer.fit_transform(np.array(IMDB_train_rawlines))\n",
    "IMDB_valid_x = IMDB_vectorizer.transform(np.array(IMDB_valid_rawlines))\n",
    "IMDB_test_x = IMDB_vectorizer.transform(np.array(IMDB_test_rawlines))\n",
    "IMDB_train_y = np.array(IMDB_train_y)\n",
    "IMDB_valid_y = np.array(IMDB_valid_y)\n",
    "IMDB_test_y = np.array(IMDB_test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "randomClassifier = DummyClassifier(strategy=\"uniform\")\n",
    "majorityClassifier = DummyClassifier(strategy=\"most_frequent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DummyClassifier(constant=None, random_state=None, strategy='most_frequent')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "randomClassifier.fit(IMDB_bbow_train_x, IMDB_bbow_train_y)\n",
    "majorityClassifier.fit(IMDB_bbow_train_x, IMDB_bbow_train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5051587618971446\n"
     ]
    }
   ],
   "source": [
    "IMDB_bbow_test_pred = randomClassifier.predict(IMDB_bbow_test_x)\n",
    "print(f1_score(IMDB_bbow_test_y,IMDB_bbow_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "IMDB_bbow_test_pred = majorityClassifier.predict(IMDB_bbow_test_x)\n",
    "print(f1_score(IMDB_bbow_test_y,IMDB_bbow_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import parfit.parfit as pf\n",
    "from sklearn.model_selection import ParameterGrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB, GaussianNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_nb = ParameterGrid({\n",
    "\t'alpha' : np.logspace(0,3,10)\n",
    "})\n",
    "params_svc = ParameterGrid({\n",
    "\t'loss' :\n",
    "})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
